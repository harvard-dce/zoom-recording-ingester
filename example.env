# should only be necessary if you have multiple credential profiles
#AWS_PROFILE=

# tags that will be assigned to the cfn stack and all resources under it
# space-separated key/value pairs, like so:
# Key=key1,Value=value1 Key=key2,Value=value2
STACK_TAGS=Key=project,Value=MH Key=department,Value=DE

# name of the cfn stack; also used to prefix many resource names (required)
STACK_NAME=

# s3 bucket to store packaged lambda code
LAMBDA_CODE_BUCKET=

# where cloudwatch alarm notifications will get sent
NOTIFICATION_EMAIL=

ZOOM_API_BASE_URL=
# Either a zoom api key/secret or an apigee key. If both are present then
# apigee will be prefered.
ZOOM_API_KEY=
ZOOM_API_SECRET=
APIGEE_KEY=

# URL and API auth for the target opencast cluster
OC_CLUSTER_NAME=
OPENCAST_API_USER=
OPENCAST_API_PASSWORD=

# id of series to receive ingests in the absence of a zoom meeting -> series mapping
# this is meant for testing/dev only. If you set this to a valid Opencast series id
# then any recording that doesn't match something in the class schedule data will
# be ingested into that default series id.
DEFAULT_SERIES_ID=

# Opencast 5.x:
#     publisher = producer username or email
#     no contributor
# When the publisher of a series cannot be determined via an
# /otherpubs/epidodedefault lookup, the Opencast workflow notifications will go
# to this address. If this is left empty the notifications will go to the
# NOTIFICATION_EMAIL address.
DEFAULT_PUBLISHER=
# These settings will override the Opencast lookup of publisher and contributor
# for a series.
OVERRIDE_PUBLISHER=
OVERRIDE_CONTRIBUTOR=

# 5x clusters: "dce-int-production-zoom"
DEFAULT_OC_WORKFLOW=

# controls how many download queue messages will be processed (not ingested!) per
# invocation of the downloader function
DOWNLOAD_MESSAGES_PER_INVOCATION=10

# controls how far in minutes the schedule matching will allow for start/end times
BUFFER_MINUTES=30

# videos shorter than this many minutes will be ignored
MINIMUM_DURATION=2

# Python pytz timezone
LOCAL_TIME_ZONE=US/Eastern

LOG_NOTIFICATIONS_FILTER_LOG_LEVEL=ERROR

# the uploader will query opencast for the number of currently running track uploads
# if it is greater than this number the uploader will abort (leaving the upload in the queue)
OC_TRACK_UPLOAD_MAX=5

# These (comma-separated) IPs will be included in the APIs resource policy
# Only requests coming from these IPs will be allowed to exec the on-demand ingest endpoint
# Assuming the Opsworks cluster is up-to-date, these should be the same ips listed
# in the cluster config's "vpn_ips" list.
INGEST_ALLOWED_IPS=

# Google sheets
# The document id
GSHEETS_DOC_ID =
# Comma delimited list of the names of tabs to download as a csv
GSHEETS_SHEET_NAMES=

# Create a custom Slack app at https://api.slack.com/
# Creating an app generates a signing secret
# View your custom app's signing secret in Slack "App Credentials" section
# Slack signing secret is used to verify requests from Slack
SLACK_SIGNING_SECRET=
# The only channel in which outputs of the slack app will be visible to others
# Preferably a private Slack channel
SLACK_ZIP_CHANNEL=
# Which Slack usergroups are allowed to use this app
SLACK_ALLOWED_GROUP=
# A Slack API token for reading usergroup lists
SLACK_API_TOKEN=
